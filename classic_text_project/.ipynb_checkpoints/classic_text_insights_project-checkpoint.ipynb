{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover Insights into Classic Texts\n",
    "\n",
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project we will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: Oscar Wilde’s The Picture of Dorian Gray or Homer’s The Iliad! One of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!\n",
    "\n",
    "By the end of this project, we will find out the main topics of discussion in the novel of choosing and can begin to discern some of the author’s thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import and Preprocess Text Data\n",
    "\n",
    "There are text files for the *The Picture of Dorian Gray*, named `dorian_gray.txt`, and *The Iliad*, named `the_iliad.txt`, sourced from Project Gutenberg. Let's import either of the text and convert it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/awesomeness_a/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# import text\n",
    "text = open('dorian_gray.rtf', encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the text imported, now we need to split the text into individual sentences and then individual words. This allows us to perform a sentence-by-sentence parsing analysis!\n",
    "\n",
    "`word_sentence_tokenize()` will tokenize a text and then word tokenize each sentence, returning a list of word tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'grass', ',', 'white', 'daisies', 'were', 'tremulous.\\\\', '\\\\', 'after', 'a', 'pause', ',', 'lord', 'henry', 'pulled', 'out', 'his', 'watch', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence and word tokenize text\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence\n",
    "single_word_tokenized_sentence = word_tokenized_text[100]\n",
    "print(single_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Part-of-speech Tag Text\n",
    "\n",
    "Next we will part-of-speech tag each sentence to allow for syntax parsing! We begin by creating a list named `pos_tagged_text` that will hold each part-of-speech tagged sentence from the novel.\n",
    "\n",
    "Then, we loop through each word tokenized sentence in `word_tokenized_text` and part-of-speech tag each sentence using nltk‘s `pos_tag()` function. After that, we append the result to `pos_tagged_text`.\n",
    "\n",
    "We also save any part-of-speech tagged sentence in `pos_tagged_text` to a variable named `single_pos_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'PRP'), ('is', 'VBZ'), ('better', 'RBR'), ('not', 'RB'), ('to', 'TO'), ('be', 'VB'), ('different', 'JJ'), ('from', 'IN'), (\"one's\\\\\", 'JJ'), ('fellows', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold part-of-speech tagged sentences\n",
    "pos_tagged_text = list()\n",
    "\n",
    "# create a for loop through each word tokenized sentence\n",
    "for token in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences\n",
    "  pos_tagged_text.append(pos_tag(token))\n",
    "\n",
    "# store and print any part-of-speech tagged sentence\n",
    "single_pos_sentence = pos_tagged_text[67]\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Chunk Sentences\n",
    "\n",
    "Now that we have part-of-speech tagged our text, we can move on to *syntax parsing*!\n",
    "\n",
    "We begin by defining a piece of chunk grammar `np_chunk_grammar` that will chunk a noun phrase. A noun phrase consists of an optional determiner `DT`, followed by any number of adjectives `JJ`, followed by a noun `NN`.\n",
    "\n",
    "Then, we create a nltk RegexpParser object named `np_chunk_parser` using the noun phrase chunk grammar we defined as an argument.\n",
    "\n",
    "We define a piece of chunk grammar named `vp_chunk_grammar` that will chunk a verb phrase of the following form: noun phrase, followed by a verb `VB`, followed by an optional adverb `RB`.\n",
    "\n",
    "After that, we create a nltk RegexpParser object named `vp_chunk_parser` using the verb phrase chunk grammar we defined as an argument.\n",
    "\n",
    "`np_chunked_text` and `vp_chunked_text` will hold the chunked sentences from the text.\n",
    "\n",
    "We loop through each part-of-speech tagged sentence in `pos_tagged_text` and noun phrase chunk each sentence using RegexpParser‘s `.parse()` method, and append the result to `np_chunked_text`.\n",
    "\n",
    "Within the same loop, we verb phrase chunk each part-of-speech tagged sentence using RegexpParser‘s `.parse()` method, and append the result to `vp_chunked_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noun phrase chunk grammar\n",
    "np_chunk_grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "# create noun phrase RegexpParser object\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar\n",
    "vp_chunk_grammar = 'VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}'\n",
    "\n",
    "# create verb phrase RegexpParser object\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences\n",
    "np_chunked_text = list()\n",
    "vp_chunked_text = list()\n",
    "\n",
    "\n",
    "# create a for loop through each pos-tagged sentence\n",
    "for sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(sentence))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Analyze Chunks\n",
    "\n",
    "Now that we have chunked the novel, we can analyze the chunk frequencies to gain insights!\n",
    "\n",
    "We need to use a function `np_chunk_counter()` that returns the 30 most common NP-chunks from a list of chunked sentences. \n",
    "\n",
    "Let's call `np_chunk_counter()` with `np_chunked_text` as an argument.\n",
    "\n",
    "Let's also use `vp_chunk_counter()` that returns the 30 most common VP-chunks from a list of chunked sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 907), ((('\\\\', 'JJ'), ('\\\\', 'NN')), 787), ((('\\\\', 'NN'),), 333), ((('lord', 'NN'),), 184), ((('henry', 'NN'),), 180), ((('life', 'NN'),), 156), ((('harry', 'NN'),), 137), ((('something', 'NN'),), 117), ((('dorian', 'JJ'), ('gray', 'NN')), 117), ((('the\\\\', 'NN'),), 94), ((('he\\\\', 'NN'),), 88), ((('nothing', 'NN'),), 86), ((('basil', 'NN'),), 80), ((('anything', 'NN'),), 65), ((('the', 'DT'), ('world', 'NN')), 62), ((('hallward', 'NN'),), 61), ((('everything', 'NN'),), 60), ((('i\\\\', 'NN'),), 56), ((('the', 'DT'), ('man', 'NN')), 54), ((('love', 'NN'),), 53), ((('art', 'NN'),), 52), ((('the', 'DT'), ('room', 'NN')), 50), ((('dorian', 'NN'),), 50), ((('face', 'NN'),), 46), ((('course', 'NN'),), 46), ((('it\\\\', 'NN'),), 46), ((('the', 'DT'), ('door', 'NN')), 46), ((('and\\\\', 'NN'),), 46), ((('that\\\\', 'NN'),), 45), ((('round', 'NN'),), 42)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common NP-chunks\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), ('am', 'VBP')), 97), ((('i', 'NN'), ('was', 'VBD')), 37), ((('i', 'NN'), ('want', 'VBP')), 33), ((('i', 'NN'), ('know', 'VBP')), 32), ((('i', 'NN'), ('have', 'VBP')), 30), ((('i', 'NN'), ('had', 'VBD')), 28), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 14), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 13), ((('he\\\\', 'NN'), ('had', 'VBD')), 13), ((('henry', 'NN'), ('had', 'VBD')), 12), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 12), ((('\\\\', 'NN'), ('\\\\', 'VBZ')), 12), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('it\\\\', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('believe', 'VBP')), 10), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 10), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('thought', 'VBD')), 9), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 8), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('said', 'VBD')), 8), ((('life', 'NN'), ('has', 'VBZ')), 8), ((('i', 'NN'), ('see', 'VBP')), 7), ((('i', 'NN'), ('have\\\\', 'VBP')), 6), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 6), ((('i', 'NN'), ('suppose', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('that\\\\', 'NN'), ('was', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common VP-chunks\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `most_common_np_chunks`, we can identify characters of importance in the text such as `henry`, `harry`, `dorian gray`, and `basil`, based on their frequency. Additionally another noun phrase `the picture` appears to be very relevant.\n",
    "\n",
    "Looking at `most_common_vp_chunks`, some interesting findings appear. The verb phrases `i want`, `i know` and `i have` occur frequently, indicating a theme of desire and need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
