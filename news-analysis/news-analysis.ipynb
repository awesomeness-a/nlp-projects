{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the News Analysis\n",
    "\n",
    "Newspapers and their online formats supply the public with the information we need to understand the events occurring in the world around us. From politics to sports, the news keeps us informed, in the loop, and ready to make decisions about how to act in a rapidly changing world.\n",
    "\n",
    "Given the vast amount of news articles in circulation, identifying and organizing articles by topic is a useful activity. This can help us sift through the enormous amount of information out there so we can find the news relevant to our interests, or even allow us to build a news recommendation engine!\n",
    "\n",
    "[The News International](https://www.thenews.com.pk/) is the largest English language newspaper in Pakistan, covering local and international news across a variety of sectors. A selection of articles from a [Kaggle Dataset of The News International articles](https://www.kaggle.com/asad1m9a9h6mood/news-articles) is provided in the workspace.\n",
    "\n",
    "In this project we will use term frequency-inverse document frequency (tf-idf) to analyze each article’s content and uncover the terms that best describe each article, providing quick insight into each article’s topic.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation\n",
    "\n",
    "In order to calculate tf-idf scores for the articles in the news dataset, we need to import `CountVectorizer`, `TfidfTransformer`, and `TfidfVectorizer` from `sklearn.feature_extraction.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from articles import articles\n",
    "from preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided in `articles.py` is a selection of 10 articles from The News International. Each article, stored as a string, is given as a corpus in the list articles.\n",
    "\n",
    "Let's print one of the articles and read its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARACHI: Wholesale market rates for sugar dropped to less than Rs 50 per kg following the resumption of sugar cane crushing by sugar mills in Sindh. Within two days, the rate dropped by Rs 1.70 to Rs 49.80 per kg in Karachi Whole Sale Market. According to dealers, the resumption of sugar cane crushing by the mills stabilised the supply to the market with an immediate effect on price as well. Industry experts said that the quality of sugar cane is excellent in Sindh and approximately 100 kg of sugar cane can produce 11 kg of sugar.\n"
     ]
    }
   ],
   "source": [
    "# view article\n",
    "print(articles[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let’s preprocess each article by performing _tokenization_ and _lemmatization_.\n",
    "\n",
    "We'll use a function `preprocess_text()` (imported from `preprocessing.py`) that accepts a string as input and returns a preprocessed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islamabad long queue of vehicle on fuel station be visible in different part of the country a the petrol become rare commodity on thursday federal minister for petroleum shahid khaqan abbasi say it may take up to ten day to bring the situation to normality he claim that northern area of pakistan have be face the petrol shortage the minister cite the recent decline in petroleum price and delay in a shipment a reason for the shortage he say situation would improve a soon a shipment reach pakistan source tell geo news hat due to financial restraint the pakistan state oil have be unable import petrol\n"
     ]
    }
   ],
   "source": [
    "# preprocess each article in articles and store the processed articles in a list called processed_articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "\n",
    "# print out one of the preprocessed articles\n",
    "print(processed_articles[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Tf-idf Scores\n",
    "\n",
    "We want to begin analysis by starting off with simple word counts for each article, so we initialize a CountVectorizer object assigned to a variable named `vectorizer`.\n",
    "\n",
    "Then, we fit and transform our vectorizer on `processed_articles` to get the word counts for each article. The resulting counts are saved to a variable named `counts`.\n",
    "\n",
    "After we saved the word counts to counts, we can see a DataFrame with the word counts for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi          0          0          0          1          0          0   \n",
      "abide           1          0          0          0          0          0   \n",
      "about           0          0          0          0          0          0   \n",
      "accord          0          0          1          0          0          0   \n",
      "add             1          0          0          0          0          0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world           0          0          0          0          0          3   \n",
      "would           0          0          0          1          0          0   \n",
      "year            0          1          0          0          0          0   \n",
      "yi              0          0          0          0          0          0   \n",
      "yuan            0          0          0          0          0          0   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi          0          0          0           0  \n",
      "abide           0          0          0           0  \n",
      "about           1          0          0           0  \n",
      "accord          0          0          0           0  \n",
      "add             0          0          1           0  \n",
      "...           ...        ...        ...         ...  \n",
      "world           0          0          0           0  \n",
      "would           0          0          1           0  \n",
      "year            0          0          0           0  \n",
      "yi              0          0          0           2  \n",
      "yuan            0          0          0           2  \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_word_counts)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the word counts for each article, let’s convert them into tf-idf scores.\n",
    "\n",
    "We need to initialize a `TfidfTransformer` object with keyword argument `norm=None` and save it to a variable `transformer`.\n",
    "\n",
    "Then, we need to fit and transform our transformer on counts to convert the word counts into tf-idf scores for each article. The resulting tf-idf scores are saved to a variable named `tfidf_scores_transformed`.\n",
    "\n",
    "After that, we can see another DataFrame that contains the tf-idf scores for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
      "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
      "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
      "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
      "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    2.704748        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   2.299283    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   2.299283    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    5.409496  \n",
      "yuan     0.000000        0.0   0.000000    5.409496  \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# convert counts to tf-idf\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now ve have tf-idf scores for each article. But we want to confirm, however, that the TfidfTransformer gives the same results as directly using the TfidfVectorizer.\n",
    "\n",
    "To do that, we start with initializing a `TfidfVectorizer` object with keyword argument `norm=None` saved to a variable `vectorizer`.\n",
    "\n",
    "Then, we fit and transform the vectorizer on `processed_articles` to calculate the tf-idf scores for each article in one step. The resulting tf-idf scores are saved to a variable named `tfidf_scores`.\n",
    "\n",
    "Now we can see another DataFrame appear in the browser component. Do the tf-idf scores given by TfidfVectorizer look the same as those given by TfidfTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
      "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
      "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
      "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
      "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    2.704748        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   2.299283    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   2.299283    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    5.409496  \n",
      "yuan     0.000000        0.0   0.000000    5.409496  \n",
      "\n",
      "[353 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s confirm that the tf-idf scores given by TfidfTransformer and TfidfVectorizer are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Are the tf-idf scores the same?\n",
      "0                             YES\n"
     ]
    }
   ],
   "source": [
    "# check if tf-idf scores are equal\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?': ['YES']}))\n",
    "else:\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?': ['No, something is wrong :(']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results\n",
    "\n",
    "A simple way of identifying the “topic” of a document is to label the document with its highest-scoring tf-idf term. While this is a more naive approach than others, it is a quick and easy way of getting insight into the topic of a document.\n",
    "\n",
    "Let's write a for loop that iterates a variable i through the values 1 to 10.\n",
    "\n",
    "The `Pandas` Series method `.idxmax()` is a helpful tool for returning the index of the highest value in a DataFrame column. We will use this method to find the highest scoring tf-idf term for each article.\n",
    "\n",
    "On each pass through the for loop, we will print the index of the term with the highest tf-idf score for that article (from Article 1 to Article 10).\n",
    "\n",
    "Compare the actual text of the articles to the selected term. Do printed terms give any insight into the topic of the respective articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1    fare\n",
      "dtype: object\n",
      "Article 2    hong\n",
      "dtype: object\n",
      "Article 3    sugar\n",
      "dtype: object\n",
      "Article 4    petrol\n",
      "dtype: object\n",
      "Article 5    engine\n",
      "dtype: object\n",
      "Article 6    australia\n",
      "dtype: object\n",
      "Article 7    car\n",
      "dtype: object\n",
      "Article 8    railway\n",
      "dtype: object\n",
      "Article 9    cabinet\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get highest scoring tf-idf term for each article\n",
    "for i in range(1, 10):\n",
    "  print(df_tf_idf[[f'Article {i}']].idxmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
